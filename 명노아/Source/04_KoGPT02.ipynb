{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\TEXT_017_220_38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['â–ì•ˆë…•',\n",
       " 'í•˜',\n",
       " 'ì„¸',\n",
       " 'ìš”.',\n",
       " 'â–í•œêµ­ì–´',\n",
       " 'â–G',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " 'â–ì…',\n",
       " 'ë‹ˆë‹¤.',\n",
       " 'ğŸ˜¤',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",bos_token='</s>', eos_token='</s>', unk_token='<unk>',pad_token='<pad>', mask_token='<mask>')\n",
    "tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì ëœ° TVë¥¼ í†µí•´ì„œë„ ë³¼ ìˆ˜ ìˆë‹¤.\n",
      "ì´ë²ˆ ì „ì‹œëŠ” ì§€ë‚œí•´ 11ì›”ë¶€í„° ì˜¬ 1ì›”ê¹Œì§€ ì§„í–‰í•œ '2018 ì„œìš¸êµ­ì œê´‘ê³ ì œ'ì—ì„œ ìˆ˜ìƒí•œ ì‘í’ˆë“¤ë¡œ êµ¬ì„±ëë‹¤.\n",
      "ì „ì‹œì—ëŠ” ì´ 4000ì—¬ ì ì˜ ì‘í’ˆì´ ì¶œí’ˆëìœ¼ë©° ì´ì¤‘ ì•½ 1000ì ì´ ë„˜ëŠ” ì‘í’ˆë“¤ì´ ì†Œê°œëœë‹¤.\n",
      "íŠ¹íˆ ì´ë²ˆ ì „ì‹œíšŒì—ì„œëŠ” êµ­ë‚´ ìµœì´ˆë¡œ ì„ ë³´ì´ëŠ” ë¯¸ë””ì–´ì•„íŠ¸ ì‘í’ˆì„ ë¹„ë¡¯í•´ ë‹¤ì–‘í•œ ì¥ë¥´ì˜ ê´‘ê³ ì‘í’ˆë“¤ì´ ëŒ€ê±° ê³µê°œë¼ ëˆˆê¸¸ì„ ëˆë‹¤.\n",
      "ë¯¸ë””ì–´ ì•„íŠ¸ëŠ” í˜„ëŒ€ì¸ì˜ ì¼ìƒì„ í‘œí˜„í•˜ëŠ” ì˜ˆìˆ  ì¥ë¥´ë¡œ ìµœê·¼ ì£¼ëª©ë°›ê³  ìˆëŠ” ë””ì§€í„¸ ê¸°ìˆ ì„ í™œìš©í•´ ìƒˆë¡œìš´ í˜•íƒœì˜ ê´‘ê³ ë¥¼ ì œì‘í•˜ê³  ì´ë¥¼ í™œìš©í•œ ë§ˆì¼€íŒ…ì„ ì‹œë„í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤.\n",
      "ë˜í•œ, ë¯¸ë””ì–´ëŠ” ì¸ê°„ì˜ ì‚¶ì„ í’ìš”ë¡­ê²Œ í•˜ëŠ” ë§¤ê°œì²´ë¡œ ì¸ì‹ë˜ê³  ìˆìœ¼ë©°, ì´ëŸ¬í•œ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ëŠ” ë§¤ì²´\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "text = 'ì ëœ° TV'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_017_220_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
